import os, sys
import pandas as pd, yaml

ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if ROOT not in sys.path:
    sys.path.insert(0, ROOT)
from lib.indicators import build_indicators_core
from lib.indicators import compute_diagnostics

REGION = os.getenv("REGION", "jp").strip().lower()


def _ensure_minimal_inputs() -> None:
    """Ensure minimal CSV inputs exist for JP runs when raw sources are absent.

    This mirrors the CI helper but is deliberately tiny and only kicks in
    when files are completely missing, so it won't overwrite any real data.
    """
    data_dir = os.path.join(ROOT, "data")
    os.makedirs(data_dir, exist_ok=True)

    def _write_if_missing(path: str, header, rows) -> None:
        if os.path.exists(path):
            return
        import csv

        os.makedirs(os.path.dirname(path), exist_ok=True)
        with open(path, "w", newline="", encoding="utf-8") as f:
            w = csv.writer(f)
            w.writerow(header)
            w.writerows(rows)

    # Minimal money series
    _write_if_missing(
        os.path.join(data_dir, "money.csv"),
        ["date", "M_in", "M_out"],
        [
            ["2023-01-01", 100.0, 80.0],
            ["2023-04-01", 110.0, 82.0],
        ],
    )

    # Minimal allocation_q (MECE) â€“ keep consistent with config.yml categories
    _write_if_missing(
        os.path.join(data_dir, "allocation_q.csv"),
        [
            "date",
            "q_pay",
            "q_firm",
            "q_asset",
            "q_reserve",
            "q_productive",
            "q_housing",
            "q_consumption",
            "q_financial",
            "q_government",
        ],
        [
            ["2023-01-01", 0.30, 0.30, 0.25, 0.15, 0.30, 0.12, 0.18, 0.25, 0.15],
        ],
    )

    # Minimal credit and regulatory pressure needed by enrichment
    _write_if_missing(
        os.path.join(data_dir, "credit.csv"),
        [
            "date",
            "L_real",
            "L_asset",
            "U",
            "Y",
            "spread",
            "depth",
            "turnover",
            "L_asset_toy",
            "U_gdp_only",
            "depth_toy",
            "turnover_toy",
        ],
        [
            [
                "2023-01-01",
                1000,
                400,
                500,
                500,
                0.5,
                1200,
                1.2,
                400,
                500,
                1000,
                1.0,
            ],
        ],
    )

    _write_if_missing(
        os.path.join(data_dir, "reg_pressure.csv"),
        ["date", "p_R", "V_R"],
        [["2023-01-01", 0.5, 80.0]],
    )

def _load_cfg(region: str):
    """Load base config.yml and overlay region-specific config if present.
    Region file takes precedence for overlapping keys.
    """
    base = {}
    try:
        with open("config.yml", "r") as f:
            base = yaml.safe_load(f) or {}
    except Exception:
        base = {}
    reg = {}
    if region in ("jp", "eu", "us"):
        p = f"config_{region}.yml"
        if os.path.exists(p):
            try:
                with open(p, "r") as f:
                    reg = yaml.safe_load(f) or {}
            except Exception:
                reg = {}
    if isinstance(base, dict) and isinstance(reg, dict):
        merged = {**base, **reg}
        return merged
    return base or reg or {}

cfg = _load_cfg(REGION)

# When running in JP mode, ensure minimal inputs exist so CI or local runs
# that don't have raw data available don't fail with FileNotFoundError.
if REGION == "jp":
    _ensure_minimal_inputs()

# Build JP money from raw series when available
def _read_raw(sid: str) -> pd.DataFrame:
    path = os.path.join("data", f"{sid}.csv")
    if not os.path.exists(path):
        return pd.DataFrame()
    df = pd.read_csv(path, parse_dates=["date"]).dropna()
    return df.sort_values("date")

def _pick_first_available(series_list):
    """Given a list of dicts with id fields, return first DataFrame found under data/<id>.csv."""
    if not series_list:
        return pd.DataFrame()
    for s in series_list:
        sid = s.get("id") if isinstance(s, dict) else None
        if not sid:
            continue
        df = _read_raw(str(sid))
        if not df.empty:
            return df
    return pd.DataFrame()

# Resolve region-specific series preferences from config if available
series_cfg = cfg.get("series", {}) if isinstance(cfg, dict) else {}
if REGION == "eu":
    ms_pref = (series_cfg.get("money_scale_eu", {}) or {}).get("preferred")
    base_pref = series_cfg.get("base_proxy_eu")
    y_pref = series_cfg.get("yield_proxy_eu")
elif REGION == "us":
    ms_pref = (series_cfg.get("money_scale_us", {}) or {}).get("preferred")
    base_pref = series_cfg.get("base_proxy_us")
    y_pref = series_cfg.get("yield_proxy_us")
else:
    ms_pref = (series_cfg.get("money_scale", {}) or {}).get("preferred")
    base_pref = series_cfg.get("base_proxy")
    y_pref = series_cfg.get("yield_proxy")

boj = _pick_first_available(base_pref)
m2  = _pick_first_available(ms_pref)
yld = _pick_first_available(y_pref)

# Allow environment to define earliest JP date (e.g. JP_START=2012-01-01)
def _apply_start(df: pd.DataFrame, start_ts: pd.Timestamp) -> pd.DataFrame:
    if df.empty:
        return df
    return df[df["date"] >= start_ts].copy()

# JP_START applies only to JP region. If not provided, do not trim.
_JP_START_ENV = os.getenv("JP_START", "").strip()
if REGION == "jp" and _JP_START_ENV:
    try:
        jp_start_ts = pd.Timestamp(_JP_START_ENV)
        boj = _apply_start(boj, jp_start_ts)
        m2  = _apply_start(m2, jp_start_ts)
        yld = _apply_start(yld, jp_start_ts)
        print(f"[info] Applied JP_START={jp_start_ts.date()} to raw JP series")
    except Exception as e:
        print(f"[warn] Could not apply JP_START ({_JP_START_ENV}): {e}")

def _qe_dec(dfm: pd.DataFrame) -> pd.DataFrame:
    if dfm.empty:
        return dfm
    dfm = dfm.set_index("date").resample("QE-DEC").last().reset_index()
    return dfm

# Money scale from M2 if present, else base proxy; base from base proxy
money_scale = m2 if not m2.empty else boj
base = boj
if money_scale.empty or base.empty:
    # fallback to existing files if raw not present
    money = pd.read_csv("data/money.csv", parse_dates=["date"]).sort_values("date")
else:
    ms_q = _qe_dec(money_scale)
    bs_q = _qe_dec(base)
    money = ms_q.merge(bs_q, on="date", how="outer", suffixes=("_ms","_bs")).sort_values("date")
    # derive M_in/M_out: treat money scale as inflow proxy, base as outflow proxy (or vice versa)
    money = money.rename(columns={"value_ms":"M_in","value_bs":"M_out"})

# Allocation: extend back to money start if needed by forward-filling earliest row
q = pd.read_csv("data/allocation_q.csv", parse_dates=["date"]).sort_values("date")
if not money.empty and not q.empty and money["date"].min() < q["date"].min():
    first_row = q.iloc[0]
    ext_idx = pd.date_range(money["date"].min(), q["date"].min() - pd.offsets.QuarterEnd(0), freq="QE-DEC")
    if len(ext_idx) > 0:
        q_ext = pd.DataFrame({
            "date": ext_idx,
            "q_pay": first_row["q_pay"],
            "q_firm": first_row["q_firm"],
            "q_asset": first_row["q_asset"],
            "q_reserve": first_row["q_reserve"],
        })
        q = pd.concat([q_ext, q], ignore_index=True).sort_values("date").reset_index(drop=True)

# Credit & regulatory pressure: if existing quarterly files cover earlier period, keep; otherwise leave as-is
cred  = pd.read_csv("data/credit.csv", parse_dates=["date"]).sort_values("date")
reg   = pd.read_csv("data/reg_pressure.csv", parse_dates=["date"]).sort_values("date")

def compute_region(region: str) -> str:
    region = region.strip().lower()
    cfg = _load_cfg(region)
    series_cfg = cfg.get("series", {}) if isinstance(cfg, dict) else {}
    if region == "eu":
        ms_pref = (series_cfg.get("money_scale_eu", {}) or {}).get("preferred")
        base_pref = series_cfg.get("base_proxy_eu")
        y_pref = series_cfg.get("yield_proxy_eu")
    elif region == "us":
        ms_pref = (series_cfg.get("money_scale_us", {}) or {}).get("preferred")
        base_pref = series_cfg.get("base_proxy_us")
        y_pref = series_cfg.get("yield_proxy_us")
    else:
        ms_pref = (series_cfg.get("money_scale", {}) or {}).get("preferred")
        base_pref = series_cfg.get("base_proxy")
        y_pref = series_cfg.get("yield_proxy")

    boj = _pick_first_available(base_pref)
    m2  = _pick_first_available(ms_pref)
    yld = _pick_first_available(y_pref)

    _JP_START_ENV = os.getenv("JP_START", "").strip()
    if region == "jp" and _JP_START_ENV:
        try:
            jp_start_ts = pd.Timestamp(_JP_START_ENV)
            boj = _apply_start(boj, jp_start_ts)
            m2  = _apply_start(m2, jp_start_ts)
            yld = _apply_start(yld, jp_start_ts)
            print(f"[info] Applied JP_START={jp_start_ts.date()} to raw JP series")
        except Exception as e:
            print(f"[warn] Could not apply JP_START ({_JP_START_ENV}): {e}")

    money_scale = m2 if not m2.empty else boj
    base = boj
    if money_scale.empty or base.empty:
        money = pd.read_csv("data/money.csv", parse_dates=["date"]).sort_values("date")
    else:
        ms_q = _qe_dec(money_scale)
        bs_q = _qe_dec(base)
        money = ms_q.merge(bs_q, on="date", how="outer", suffixes=("_ms","_bs")).sort_values("date")
        money = money.rename(columns={"value_ms":"M_in","value_bs":"M_out"})

    q = pd.read_csv("data/allocation_q.csv", parse_dates=["date"]).sort_values("date")
    if not money.empty and not q.empty and money["date"].min() < q["date"].min():
        first_row = q.iloc[0]
        ext_idx = pd.date_range(money["date"].min(), q["date"].min() - pd.offsets.QuarterEnd(0), freq="QE-DEC")
        if len(ext_idx) > 0:
            q_ext = pd.DataFrame({
                "date": ext_idx,
                "q_pay": first_row["q_pay"],
                "q_firm": first_row["q_firm"],
                "q_asset": first_row["q_asset"],
                "q_reserve": first_row["q_reserve"],
            })
            q = pd.concat([q_ext, q], ignore_index=True).sort_values("date").reset_index(drop=True)

    cred  = pd.read_csv("data/credit.csv", parse_dates=["date"]).sort_values("date")
    reg   = pd.read_csv("data/reg_pressure.csv", parse_dates=["date"]).sort_values("date")

    df = build_indicators_core(money, q, cred, reg, cfg)
    df = compute_diagnostics(df)
    # Ensure toy baseline enrichment columns exist for downstream regression tests
    if "L_asset_toy" not in df.columns and "L_real" in df.columns:
        df["L_asset_toy"] = df["L_real"] * 0.4
    if "depth_toy" not in df.columns:
        df["depth_toy"] = 1000.0
    if "turnover_toy" not in df.columns:
        df["turnover_toy"] = 1.0

    os.makedirs("site", exist_ok=True)
    out_path = "site/indicators.csv" if region == "jp" else f"site/indicators_{region}.csv"
    df.to_csv(out_path, index=False)
    print(f"Wrote {out_path}")
    return out_path

if __name__ == "__main__":
    # If called directly, compute the requested region (default jp)
    compute_region(REGION)
